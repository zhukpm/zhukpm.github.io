<!DOCTYPE html>

<html lang="en">
<head>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-N19THDZVEY"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-N19THDZVEY');
</script>
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.30.0/themes/prism.min.css" rel="stylesheet"/>
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.30.0/plugins/toolbar/prism-toolbar.min.css" rel="stylesheet"/>
<link href="/theme/css/style.css" rel="stylesheet"/>
<link href="/theme/css/fonts.css" rel="stylesheet"/>
<meta content="width=device-width,initial-scale=1.0" name="viewport"/>
<title>Choosing the best architecture for your text classification task – Viacheslav Zhukov</title>
<meta charset="utf-8"/>
<meta content="text classification, machine learning, NLP, LLMs, RoBERTa, fastText, ChatGPT" name="keywords"/>
<meta content="https://zhukpm.github.io/og-image.png" property="og:image"/>
<meta content="A review of text classification models based on real-world applications" name="description"/>
<meta content="LLMs" name="tags"/>
<meta content="Machine Learning" name="tags"/>
<meta content="Text Classification" name="tags"/>
<meta content="#1F3683" name="theme-color"/>
<link href="https://zhukpm.github.io/logo_rounded.png" rel="apple-touch-icon" sizes="192x192"/>
<link href="https://zhukpm.github.io/logo_rounded.png" rel="icon"/>
<script>

if ('serviceWorker' in navigator) {
  window.addEventListener('load', function() {
    navigator.serviceWorker.register('/service-worker.js').then(function(registration) {
      console.log('ServiceWorker registration successful with scope: ', registration.scope);
    }, function(err) {
      console.log('ServiceWorker registration failed: ', err);
    });
  });
}

        </script>
<link href="https://zhukpm.github.io/posts/2023/choosing-the-best-architecture-for-your-text-classification-task" rel="canonical"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": "BreadcrumbList", "itemListElement": [{"@type": "ListItem", "position": 1, "name": "Viacheslav Zhukov", "item": "https://zhukpm.github.io"}, {"@type": "ListItem", "position": 2, "name": "Posts", "item": "https://zhukpm.github.io/posts"}, {"@type": "ListItem", "position": 3, "name": "2023", "item": "https://zhukpm.github.io/posts/2023"}, {"@type": "ListItem", "position": 4, "name": "Choosing the best architecture for your text classification task", "item": "https://zhukpm.github.io/posts/2023/choosing-the-best-architecture-for-your-text-classification-task.html"}]}</script><script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "author": {"@type": "Person", "name": "Viacheslav Zhukov"}, "publisher": {"@type": "Organization", "name": "Viacheslav Zhukov", "logo": {"@type": "ImageObject", "url": "https://zhukpm.github.io/logo_rounded.png"}}, "headline": "Choosing the best architecture for your text classification&nbsp;task", "about": "Natural Language Processing", "datePublished": "2023-02-15 14:00"}</script></head>
<body class="home" id="index">
<header class="body" id="banner">
<h1><a href="https://zhukpm.github.io/">Viacheslav Zhukov</a></h1>
<p>Notes on AI, ML, Software Engineering and Math</p>
</header>
<nav id="menu"><ul>
<li><a href="/">Posts</a></li>
<li><a href="https://zhukpm.github.io/javascript-memes">Javascript Memes</a></li>
<li><a href="https://zhukpm.github.io/cv"><span class="caps">CV</span></a></li>
</ul></nav>
<main data-pagefind-body="" id="content">
<header>
<h1 class="entry-title" data-pagefind-meta="title">
<a href="https://zhukpm.github.io/posts/2023/choosing-the-best-architecture-for-your-text-classification-task" rel="bookmark" title="Permalink to Choosing the best architecture for your text classification task">Choosing the best architecture for your text classification task</a>
</h1>
</header>
<section class="entry-content key-text-content">
<p>Originally published on <a href="https://medium.com/toloka/choosing-the-best-architecture-for-your-text-classification-task-aee30ecc7870" target="_blank">Toloka blog on Medium</a>.</p>
<p><img alt="Language models, bro!" class="image-process-crisp" src="https://zhukpm.github.io/images/002/derivatives/crisp/1x/thumb.png" srcset="https://zhukpm.github.io/images/002/derivatives/crisp/1x/thumb.png 1x, https://zhukpm.github.io/images/002/derivatives/crisp/2x/thumb.png 2x, https://zhukpm.github.io/images/002/derivatives/crisp/4x/thumb.png 4x"/></p>
<p>Modern large language models (LLMs) have demonstrated the best performance for many <span class="caps">NLP</span> tasks from text classification to text generation. But are they really a “silver bullet” or a one-stop-shop solution? Can they be applied across the board? Toloka’s <span class="caps">ML</span> team faces these kinds of tasks all the time, and our answer so far is a resounding “No.” Performance is not the only factor you should be concerned about when developing a model for a real use case. And you probably don’t want to spend your entire department’s budget on it either.</p>
<p>We’ve created a practical guide on ways to solve text classification problems — depending on how much data you have, the type of data, time and budget constraints, and other factors.</p>
<h2>Approaches to text classification</h2>
<p>Let’s start off with a brief overview of potential models and solutions you could use.</p>
<h3>Old-school tf-idf models</h3>
<p>Models in this category are founded on basic stats like word count and co-occurrences. Reduced feature space is usually passed to one of the classic <span class="caps">ML</span> models like <span class="caps">SVM</span>, <span class="caps">MLP</span> or Naive Bayes. This method is easy to implement and does not require any specific libraries or accelerators — you’re good to go with one of the classic solutions like sklearn or <span class="caps">NLTK</span>. Moreover, these models can easily handle both short and long texts. Given their relatively small size, they’re highly efficient when it comes to training, deployment, and inference.</p>
<p>Nevertheless, this approach has several drawbacks, the most important one being performance. Compared to other approaches, tf-idf models rank the lowest. Additionally, you’ll have to carry out extensive preprocessing of your texts (misspellings, stopwords, punctuation, lemmatization, and more). You also need a lot of data to produce a robust model — and your data should resemble academic texts, not social media, which can contain numerous misspellings as well as slang.</p>
<h3>First embeddings and pre-trains</h3>
<p>Word embeddings are excellent for text classification since each word (or sequence of characters) is represented by a vector of numbers containing useful information about the context, use, and semantics.</p>
<p>Take for example Word2Vec, its variations and implementation within the fastText library — a binary file that you can run to achieve a desired result. While you still need a large dataset to produce solid word embeddings and to train the classifier head, with proper configuration you can significantly reduce your preprocessing efforts. As a single multiprocessing library, it can run right in the system’s console.</p>
<p>On average, it takes anywhere from 10 minutes to an hour to create a fastText model between 300 <span class="caps">MB</span> and 2 <span class="caps">GB</span>. This model can handle texts of any size, and the inference is incredibly fast given that the sole focus is on text embedding construction and processing by an <span class="caps">MLP</span>. The availability of pre-trained word embeddings for a variety of languages makes it a baseline for almost any text classification problem.</p>
<h3>Small transformers</h3>
<p>This category includes transformer-based language models such as <span class="caps">BERT</span> and RoBERTa — currently considered to be state-of-the-art <span class="caps">NLP</span>. Even though it may seem obvious that a model with 110 million parameters is “small”, and a model with 175 billion parameters is “large”, it’s not easy to distinguish between small and large transformers. Yet there are several key advantages that make transformers a great option. Namely, they’re resistant to misspellings and usually require little preprocessing compared to other models.</p>
<p>Since you probably won’t be training your own <span class="caps">BERT</span> and will likely be using a pre-trained model instead from a library or hub (like Hugging Face), you can use comparatively small datasets to create decent models. If you have a common task, and your domain is similar to one that already has a tuned version, you may only need a couple hundred or thousand samples to slightly tune the model and achieve great results. The model size usually ranges from 600 <span class="caps">MB</span> to several gigabytes. It’s also a good idea to have access to GPUs as the training may take some time to complete.</p>
<p>However, there are also some disadvantages to consider. The produced model is much slower compared to Word2Vec, so if you require real-time inference you’ll need to either use a <span class="caps">GPU</span> device or invest in model optimization (graph optimizations, <span class="caps">ONNX</span>, DeepSpeed, and others). Additionally, the length of texts that a model can handle is limited by its architecture and is usually about 512 tokens (~380 words). In practice, a more straightforward approach like taking 192 tokens from the beginning works well.</p>
<h3>LLMs</h3>
<p>It’s likely that you don’t have your own <span class="caps">LLM</span> — they’re really big! The size of the large downloadable version of T5 is about 40 <span class="caps">GB</span>. You’ll have to deploy this model somehow and inference may take time. In which case, you’ll either need to use an expensive computational cluster or opt for a service that provides an <span class="caps">API</span> like OpenAI with its <span class="caps">GPT</span>-3 model.</p>
<p>One benefit is that LLMs require little data for tuning, and you don’t need to worry about preprocessing. As a side note, approaches like zero-shot or few-shot don’t work well for text classification problems. You’ll need to either fine-tune or p-tune (prompt-tune) your model. Working with APIs is on a whole other level — you’ll need to consider internet access, data security, SLAs, pricing, and more. However, achievable performance is the biggest plus.</p>
<h2>Choosing by scenario</h2>
<p>As expected, all these approaches have their own pros and cons. Consider a variety of architectures to find a good fit for a specific real-world task. We recommend basing your decision on the actual requirements you have for your text classification problem.</p>
<p>Let’s go over some of the most common cases we’ve encountered — as well as our recommended approaches. Your text classification task will likely fall under one of these scenarios.</p>
<h3>Your goal is to create a high-performing model</h3>
<p>If performance really matters, choose a transformer. And spend some time searching for the optimal architecture and pre-trained weights, expanding your dataset, optimizing your pipeline and parameters, and so on. Also, try tuning an <span class="caps">LLM</span>, either your own or via an <span class="caps">API</span>. Just know that it will take time.</p>
<h3>You have little data available</h3>
<p>Go for LLMs or a small tuned transformer. If your task is general enough, you can leverage extensive model catalogs that are available across various hubs.</p>
<h3>You have a lot of data available</h3>
<p>Start with fastText and establish a baseline. This may be enough if your performance requirements aren’t that strict. If they are, go for the fine-tuning process of one of the small pre-trained transformers. If <span class="caps">API</span> is an option and you have a dedicated budget, you can try tuning an <span class="caps">LLM</span> too.</p>
<h3>You have privacy or security concerns about your data</h3>
<p>You don’t want your data to leave a specific contour and be logged by a third-party service. <span class="caps">API</span> is not an option until you make your logging and security concerns clear to the provider. Choose local models that you can deploy yourself according to your hardware and software setup.</p>
<h3>You have a common task and domain</h3>
<p>Someone has probably already solved the task for you and you can apply their solutions. Simply look for applicable tuned transformers. LLMs will likely work too, but we’ve noticed that previously tuned transformers can outperform LLMs if the dataset is extremely small (a couple hundred samples). The difference though is minute.</p>
<h3>Your task or data is very specific</h3>
<p>In this case, LLMs have the best performance compared to other approaches. Training an adequate small transformer is a challenge under these circumstances, and other architectures usually perform much worse.</p>
<h3>Your model will be used for online inference (under hundreds of milliseconds)</h3>
<p>Try fastText because of its speed. If you’re not satisfied with the quality, you can try using a small transformer. However, you’ll most likely have to use an optimization mechanism or deploy your model with access to a <span class="caps">GPU</span>. There are lots of ways to speed up inference with <span class="caps">BERT</span>-like models. LLMs are usually not an option here unless they’ve been optimized.</p>
<h3>Your model will be used for batch processing only</h3>
<p>Opt for a large model. While it seems straightforward at first glance, you still need an understanding of timing so that your batches don’t stack up.</p>
<h3>You’re concerned about scalability</h3>
<p>For example, your model will be widely used, and you expect high <span class="caps">RPS</span> on its endpoint. You’ll probably apply an orchestration mechanism like Kubernetes, assuming that your pods can be deployed and destroyed quickly, in which case there may be restrictions on model size (namely, image size). Therefore, fastText and small transformers are common options.</p>
<h3>You have access to a computational cluster with modern GPUs</h3>
<p>If so, you’re lucky! You can play with different types of transformers, even large ones, but the real question is, can you use a node pool with GPUs for inference? If yes, you can choose whatever you want, even LLMs. If not, you’ll probably find yourself optimizing a small transformer.</p>
<h3>You have no access to modern hardware accelerators</h3>
<p>That’s unfortunate to hear! Start with basic approaches and train a fastText model. You can also train a transformer model in this setup, but it will require a deeper understanding of optimization mechanisms. Another option is to move from classic libraries into something more specific like FasterTransformer.</p>
<h3>You have a lot of time to build a model</h3>
<p>Try any architecture, different pre-trains or parameters, and perform “loss-watching”.</p>
<h3>You have almost no time to build a model</h3>
<p>In this case, fastText and <span class="caps">API</span>-accessible LLMs are good options. If your task is popular, you can tune an appropriate small transformer with a default set of hyperparameters. Still, <span class="caps">API</span>-accessible LLMs are usually the best choice.</p>
<p>As you can see, there’s no “silver bullet” or “one-size-fits-all” approach. As a key takeaway, try to avoid looking at your text classification challenge with only performance in mind. There are other factors that are worth considering like inference and training time, budget, scalability, privacy, data type, and so on.</p>
</section>
<footer class="post-info">
    Written by Viacheslav Zhukov in <a href="https://zhukpm.github.io/posts/category/natural-language-processing">Natural Language Processing</a> on
    <time class="published" datetime="2023-02-15T14:00:00+01:00">        15th
February 2023
    </time>
</footer>
<section class="tags">
<a href="https://zhukpm.github.io/posts/tag/llms">LLMs</a>
<a href="https://zhukpm.github.io/posts/tag/machine-learning">Machine Learning</a>
<a href="https://zhukpm.github.io/posts/tag/text-classification">Text Classification</a>
</section>
<section class="related-posts">
<h3>More like this</h3>
<ul id="related-posts">
<li><a href="https://zhukpm.github.io/posts/2023/text-classification-challenge-with-extra-small-datasets-fine-tuning-versus-chatgpt">Text classification challenge with extra-small datasets: Fine-tuning versus ChatGPT</a></li>
</ul>
</section>
</main>
<aside id="sidebar">
<div class="sidebar-block">
<img alt="Photo of Viacheslav Zhukov" class="profile-photo" src="https://zhukpm.github.io/profile_photo.png"/>
<p>Doing AI &amp; ML engineering @ <a href="https://www.linkedin.com/company/toloka/" rel="noopener noreferrer" target="_blank" title="Toloka">Toloka</a></p>
<p> Occasional blogger, researcher, and math lover.</p>
<div class="social-icons">
<a href="https://www.linkedin.com/in/zhukpm/" rel="noopener noreferrer" target="_blank" title="LinkedIn profile of Viacheslav Zhukov">
<img alt="LinkedIn icon" class="social-icon" src="https://www.linkedin.com/favicon.ico"/>
</a>
<a href="https://github.com/zhukpm" rel="noopener noreferrer" target="_blank" title="GitHub profile of Viacheslav Zhukov">
<img alt="GitHub icon" class="social-icon" src="https://github.com/favicon.ico"/>
</a>
<a href="https://stackoverflow.com/users/6372685/viacheslav-zhukov" rel="noopener noreferrer" target="_blank" title="StackOverflow profile of Viacheslav Zhukov">
<img alt="StackOverflow icon" class="social-icon" src="https://stackoverflow.com/favicon.ico"/>
</a>
<a href="https://leetcode.com/perrymason/" rel="noopener noreferrer" target="_blank" title="LeetCode profile of Viacheslav Zhukov">
<img alt="LeetCode icon" class="social-icon" src="https://leetcode.com/favicon.ico"/>
</a>
</div>
</div>
<div class="sidebar-block">
<h2>Privacy</h2>
<p>
                        This site uses Google Analytics to understand visitor traffic and improve content.
                        It collects anonymous data like country, language, and pages visited - no personal
                        information. By continuing to browse, you agree to this minimal tracking.
                        <a href="https://www.google.com/search?q=is+google+analytics+tracking+safe" rel="noopener noreferrer" target="_blank">Read more</a>.</p>
</div>
<div class="sidebar-block">
<h2>Categories</h2>
<ul>
<li>
<a href="https://zhukpm.github.io/posts/category/math">Math (2)</a>
</li>
<li>
<a href="https://zhukpm.github.io/posts/category/natural-language-processing">Natural Language Processing (2)</a>
</li>
<li>
<a href="https://zhukpm.github.io/posts/category/software-engineering">Software Engineering (1)</a>
</li>
</ul>
</div>
<div class="sidebar-block">
<h2>Recent posts</h2>
<ul>
<li>
<a href="https://zhukpm.github.io/posts/2025/what-i-have-learned-during-my-1275-day-streak-on-leetcode">What I have learned during my 1275+ day streak on LeetCode</a>
</li>
<li>
<a href="https://zhukpm.github.io/posts/2025/mathematical-model-for-player-ranking">Mathematical Model for Player Ranking</a>
</li>
<li>
<a href="https://zhukpm.github.io/posts/2025/approximating-skills-of-table-tennis-players-using-normal-distribution-introduction">Approximating Skills of Table Tennis Players Using Normal Distribution. Introduction</a>
</li>
<li>
<a href="https://zhukpm.github.io/posts/2023/text-classification-challenge-with-extra-small-datasets-fine-tuning-versus-chatgpt">Text classification challenge with extra-small datasets: Fine-tuning versus ChatGPT</a>
</li>
<li>
<a href="https://zhukpm.github.io/posts/2023/choosing-the-best-architecture-for-your-text-classification-task">Choosing the best architecture for your text classification task</a>
</li>
</ul>
</div>
</aside>
<footer id="site-footer">
<p>Built with <a href="https://getpelican.com/" rel="noopener noreferrer" target="_blank">Pelican</a> using <a href="https://python.org/" rel="noopener noreferrer" target="_blank">Python</a> and <a href="https://github.com/hrw/pelican-haerwu-theme/" rel="noopener noreferrer" target="_blank">Haerwu theme</a>.</p>
<p>Copyright by Viacheslav Zhukov. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a>.</p>
</footer>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.30.0/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.30.0/plugins/autoloader/prism-autoloader.min.js"></script>
<script src="https://cdn.jsdelivr.net/combine/npm/prismjs@1.30.0/plugins/toolbar/prism-toolbar.min.js,npm/prismjs@1.30.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>
</body>
</html>