<!DOCTYPE html>

<html lang="en">
<head>
<!-- Google tag (gtag.js) -->
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-N19THDZVEY"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-N19THDZVEY');
</script>
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.30.0/themes/prism.min.css" rel="stylesheet"/>
<link href="https://cdn.jsdelivr.net/npm/prismjs@1.30.0/plugins/toolbar/prism-toolbar.min.css" rel="stylesheet"/>
<link href="/theme/css/style.css" rel="stylesheet"/>
<link href="/theme/css/fonts.css" rel="stylesheet"/>
<meta content="width=device-width,initial-scale=1.0" name="viewport"/>
<title>Mathematical Model for Player Ranking – Viacheslav Zhukov</title>
<meta charset="utf-8"/>
<meta content="Viacheslav Zhukov" property="og:site_name"/>
<meta content="table tennis, skill approximation, normal distribution, player ranking, game outcomes, mathematical modeling, maximum likelihood estimation, gradient descent" name="keywords"/>
<meta content="article" property="og:type"/>
<meta content="Mathematical Model for Player Ranking - Viacheslav Zhukov" property="og:title"/>
<meta content="https://vzhukov.dev/posts/2025/mathematical-model-for-player-ranking" property="og:url"/>
<meta content="2025-06-12T14:00:00+02:00" property="article:published_time"/>
<meta content="Viacheslav Zhukov" property="article:author"/>
<meta content="Math" property="article:section"/>
<meta (false)="" ,="" ;="" align='(screen.width"' center",="" content="In the previous post, we discussed how to model player skills using normal distributions. Now, let’s dive into the mathematical details of how we can estimate the parameters of these distributions using Maximum Likelihood Estimation. Problem definition: Having a set of players \(\mathcal{P}\) numbered from \(1\) to \(n …if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = " if="" indent="0em" linebreak="false" name="description" {=""/>
<meta (false)="" ,="" ;="" align='(screen.width"' center",="" content="In the previous post, we discussed how to model player skills using normal distributions. Now, let’s dive into the mathematical details of how we can estimate the parameters of these distributions using Maximum Likelihood Estimation. Problem definition: Having a set of players \(\mathcal{P}\) numbered from \(1\) to \(n …if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var align = " if="" indent="0em" linebreak="false" property="og:description" {=""/>
<meta content="Gradient Descent" name="tags"/>
<meta content="Gradient Descent" property="article:tag"/>
<meta content="Mathematical Modeling" name="tags"/>
<meta content="Mathematical Modeling" property="article:tag"/>
<meta content="Maximum Likelihood Estimation" name="tags"/>
<meta content="Maximum Likelihood Estimation" property="article:tag"/>
<meta content="Normal Distribution" name="tags"/>
<meta content="Normal Distribution" property="article:tag"/>
<meta content="Player Ranking" name="tags"/>
<meta content="Player Ranking" property="article:tag"/>
<meta content="Table Tennis" name="tags"/>
<meta content="Table Tennis" property="article:tag"/>
<meta content="https://vzhukov.dev/images/004/gradient_descent.png" property="og:image"/>
<meta content="#1F3683" name="theme-color"/>
<link href="https://vzhukov.dev/logo_rounded.png" rel="apple-touch-icon" sizes="192x192"/>
<link href="https://vzhukov.dev/logo_rounded.png" rel="icon"/>
<link href="https://vzhukov.dev/posts/2025/mathematical-model-for-player-ranking" rel="canonical"/><script type="application/ld+json">{"@context": "https://schema.org", "@type": "BreadcrumbList", "itemListElement": [{"@type": "ListItem", "position": 1, "name": "Viacheslav Zhukov", "item": "https://vzhukov.dev"}, {"@type": "ListItem", "position": 2, "name": "Posts", "item": "https://vzhukov.dev/posts"}, {"@type": "ListItem", "position": 3, "name": "2025", "item": "https://vzhukov.dev/posts/2025"}, {"@type": "ListItem", "position": 4, "name": "Mathematical model for player ranking", "item": "https://vzhukov.dev/posts/2025/mathematical-model-for-player-ranking.html"}]}</script><script type="application/ld+json">{"@context": "https://schema.org", "@type": "Article", "author": {"@type": "Person", "name": "Viacheslav Zhukov"}, "publisher": {"@type": "Organization", "name": "Viacheslav Zhukov", "logo": {"@type": "ImageObject", "url": "https://vzhukov.dev/logo_rounded.png"}}, "headline": "Mathematical Model for Player&nbsp;Ranking", "about": "Math", "datePublished": "2025-06-12 14:00"}</script></head>
<body class="home" id="index">
<header class="body" id="banner">
<h1><a href="https://vzhukov.dev/">Viacheslav Zhukov</a></h1>
<p>Notes on AI, ML, Software Engineering and Math</p>
</header>
<nav id="menu"><ul>
<li><a href="/">Posts</a></li>
<li><a href="https://vzhukov.dev/javascript-memes">Javascript Memes</a></li>
<li><a href="https://vzhukov.dev/cv"><span class="caps">CV</span></a></li>
</ul></nav>
<main data-pagefind-body="" id="content">
<header>
<h1 class="entry-title" data-pagefind-meta="title">
<a href="https://vzhukov.dev/posts/2025/mathematical-model-for-player-ranking" rel="bookmark" title="Permalink to Mathematical Model for Player Ranking">Mathematical Model for Player Ranking</a>
</h1>
</header>
<section class="entry-content key-text-content">
<p>This post is part 2 of the "Approximating Skills of Table Tennis Players Using Normal Distribution" series:</p>
<ol id="series_parts">
<li>
<a href="https://vzhukov.dev/posts/2025/approximating-skills-of-table-tennis-players-using-normal-distribution-introduction">Approximating Skills of Table Tennis Players Using Normal Distribution. Introduction</a>
</li>
<li class="active">
<a href="https://vzhukov.dev/posts/2025/mathematical-model-for-player-ranking">Mathematical Model for Player Ranking</a>
</li>
<li>
<a href="https://vzhukov.dev/posts/2025/fitting-the-player-ranking-model-a-maximum-likelihood-approach">Fitting the Player Ranking Model: A Maximum Likelihood Approach</a>
</li>
</ol>
<p>In the <a href="https://vzhukov.dev/posts/2025/approximating-skills-of-table-tennis-players-using-normal-distribution-introduction">previous post</a>, 
we discussed how to model player skills using normal distributions. Now, let’s dive into the 
mathematical details of how we can estimate the parameters of these distributions using Maximum Likelihood Estimation.</p>
<p><strong>Problem definition:</strong> Having a set of players <span class="math">\(\mathcal{P}\)</span> numbered from <span class="math">\(1\)</span> to <span class="math">\(n\)</span> and a set of game results 
<span class="math">\(\mathcal{D}\)</span> assign skill levels to each player modeling it with normal distribution.</p>
<h2>Definitions</h2>
<p>First, let’s start with definitions and basic observations.</p>
<p><strong>Set of players <span class="math">\(\mathcal{P}\)</span></strong></p>
<p>There are <span class="math">\(n\)</span> players playing in a table tennis tournament. We’ll refer to the set of players as <span class="math">\(\mathcal{P}\)</span>. 
For the sake of simplicity, we will assign a number between <span class="math">\(1\)</span> and <span class="math">\(n\)</span> to each player, essentially replacing players 
with the corresponding numbers in our model, so <span class="math">\({\mathcal{P}=\{1, 2, ..., n\}, |\mathcal{P}|=n}\)</span>.</p>
<p><strong>Set of game results <span class="math">\(\mathcal{D}\)</span></strong></p>
<p>First, there are no draws. Second, we don’t know the exact (or even approximate) skills of players. The only thing 
we know is who won in a single game between two players. So, each game can be modeled as a tuple 
<span class="math">\({(p_1, p_2), \;p_{1,2}\in\mathcal{P}}\)</span>, meaning that player <span class="math">\(p_1\)</span> won against player <span class="math">\(p_2\)</span>.</p>
<p>Moreover, the same players may play against each other multiple times, and their games may even end with different 
outcomes. So the set of game results can look like
</p>
<div class="math">$$
\mathcal{D}=\{(1,2), (1, 2), (2, 1), (2, 3), (4, 2), ...\}
$$</div>
<p>More formally,
</p>
<div class="math">\begin{equation}
    \mathcal{D}=\{G^k_{p_i, p_j}\;|\;p_i, p_j \in \mathcal{P}, \; k=\overline{1, m} \}, \; G^k_{p_i, p_j}=(p_i, p_j), \; |\mathcal{D}|=m
\end{equation}</div>
<p><strong>Calculating probability of a game outcome</strong></p>
<p>Each player’s skill is drawn from their specific distribution <span class="math">\(\mathcal{S}_i \sim \mathcal{N}(\mu_i,\,\sigma_i^2)\,\)</span>. 
The probability of an event <span class="math">\(G_{p_i, p_j}=(p_i, p_j)\)</span> where player <span class="math">\(p_i\)</span> wins against player <span class="math">\(p_j\)</span> can therefore be calculated as</p>
<div class="math">\begin{equation}
    P(G_{p_i, p_j}) = P(S_{p_i}&gt;S_{p_j})=P(S_{p_i}-S_{p_j}&gt;0)=1 - P(S_{p_i}-S_{p_j}\leq 0)
\end{equation}</div>
<p>Since <span class="math">\(S_{p_i} \sim \mathcal{N}(\mu_{p_i},\,\sigma_{p_i}^2)\,\)</span> and <span class="math">\(S_{p_j} \sim \mathcal{N}(\mu_{p_j},\,\sigma_{p_j}^2)\,\)</span>, it’s <em>quite easy</em> to show that 
<span class="math">\(Q_{p_ip_j} = S_{p_i}-S_{p_j}\sim \mathcal{N}({\mu_{p_i}-\mu_{p_j}},\,{\sigma_{p_i}^2+\sigma_{p_j}^2})\,\)</span>.</p>
<p>For standard normal distribution we know that</p>
<div class="math">\begin{equation*}
\begin{split}
    &amp; z\sim\mathcal{N}(0,\,1)\, \implies P(z\leq 0)=F_z(0)=\Phi(0), \text{where} \\
    &amp; \Phi(x)=\frac{1}{\sqrt{2\pi}}\int_{-\infty}^{x}e^{-t^2/2}dt
\end{split}
\end{equation*}</div>
<p>Here <span class="math">\(F_z(x)=P(z\leq x)\)</span> is a <a href="https://en.wikipedia.org/wiki/Cumulative_distribution_function" target="_blank">Cumulative Distribution Function (<span class="caps">CDF</span>)</a>, and <span class="math">\(\Phi(x)\)</span> is how the <span class="caps">CDF</span> of the standard 
normal distribution is usually denoted. For general normal distribution we have</p>
<div class="math">\begin{equation*}
\begin{split}
    &amp; \xi\sim\mathcal{N}(\mu,\,\sigma^2)\, \implies \xi = \sigma z + \mu \implies \\
    &amp; P(\xi\leq0)=F_{\xi}(0)=P(\sigma z + \mu \leq 0)=P\left(z \leq -\frac{\mu}{\sigma}\right) = F_z\left( -\frac{\mu}{\sigma} \right) = \Phi\left(-\frac{\mu}{\sigma}\right)
\end{split}
\end{equation*}</div>
<p>Together with the previous equation, it gives us</p>
<div class="math">\begin{equation}
\begin{split}
    &amp; P(G_{p_i, p_j}) = 1 - P(S_{p_i}-S_{p_j}\leq0) = 1-P(Q_{p_ip_j} \leq 0) = \\
    &amp; = 1-F_{Q_{p_ip_j}}(0)=1-\Phi\left(-\frac{\mu_{p_i}-\mu_{p_j}}{\sqrt{\sigma_{p_i}^2+\sigma_{p_j}^2}}\right)
\end{split}
\end{equation}</div>
<p><strong>Set of skills <span class="math">\(\mathcal{S}\)</span></strong></p>
<p>With all mentioned above, we can say that <span class="math">\(\mathcal{S}=\left\{ \mu_i \;|\; i = \overline{1, n} \right\}, \; |\mathcal{S}|= |\mathcal{P}| = n\)</span>, 
a set of <em>mean</em> skills of players. Finding <span class="math">\(\mathcal{S}\)</span> is essentially the goal of the whole process.</p>
<h3>Maximum Likelihood Estimation</h3>
<p>We will approach the problem with the <a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation" target="_blank">Maximum Likelihood Estimation (<span class="caps">MLE</span>)</a> 
method. There’s a lot of math behind it, but essentially it works as follows:</p>
<ol>
<li>We have a set of observations, a dataset.</li>
<li>We want to model it with a certain statistical model, an approach</li>
<li>This model has a set of <em>parameters</em> that define the exact behavior of the model</li>
<li>We want to find these parameters, <em>optimal values</em> for them, so we can plug them into the model and make calculations and predictions</li>
<li>We do so by <em>maximizing the probability of observing the given dataset</em>, over all possible parameter values</li>
</ol>
<p>How it all applies to our case:</p>
<ol>
<li>It’s quite clear with the dataset of observations - that’s just the set of game outcomes, <span class="math">\(\mathcal{D}\)</span>.</li>
<li>Statistical model: we say that in each game each player shows certain random skill drawn from their specific distribution, and wins against a player that showed lower skill, <em>and</em> that these distributions are modeled with normal distributions with different mean and scale values.</li>
<li>The set of parameters is, therefore, a set of mean/scale values, a total of <span class="math">\(2n\)</span> unknown parameters (since there are <span class="math">\(n\)</span> players in total). We can define it as
    <div class="math">\begin{equation*}
        \theta=
        \begin{pmatrix}
        \mu_1 &amp; \sigma_1\\
        \mu_2 &amp; \sigma_2\\
        \vdots &amp; \vdots\\
        \mu_n &amp; \sigma_n
        \end{pmatrix}
    \end{equation*}</div>
</li>
</ol>
<p>The set of parameters <span class="math">\(\theta\)</span> is our target. We find it with <span class="caps">MLE</span>:</p>
<div class="math">\begin{equation}\label{eq:mle}
    \mathcal{L}\left(\,\theta \; ; \; \mathcal{D} \,\right) = P_{\theta}\left(\,\mathcal{D} \,\right) \xrightarrow{\theta} \max
\end{equation}</div>
<p>Here come a couple of simplifications: first, we assume that games are independent of each other, they are 
<em>independent events</em>; second, as mentioned before, each player’s distribution <span class="math">\(\mathcal{S}_i\)</span> is fixed, and not 
affected by other players. With that in mind, equations above give us the following:</p>
<div class="math">\begin{equation}
    \begin{split}
    &amp;  \mathcal{L}\left(\,\theta \; ; \; \mathcal{D} \,\right) = P_{\theta}\left(\,\mathcal{D} \,\right) = P_\theta\left(\, G^0 \,G^1 \, \ldots G^m \,\right) = P_\theta\left(\, G^0 \,\right) \cdot P_\theta\left(\, G^1 \,\right) \cdot \ldots \cdot P_\theta\left(\, G^m \,\right) = \\
    &amp;= \prod_{i=1}^{m}{P_\theta\left(\, G^i \,\right) } = \prod_{i=1}^{m}{\left(1-P_\theta\left(\, S_{p_1^i} - S_{p_2^i} \leq 0 \,\right)\right)} =\\
    &amp;=\prod_{i=1}^{m}{\left(1-\Phi\left(-\frac{\mu_{\theta,\, p_1^i}-\mu_{\theta,\,p_2^i}}{\sqrt{\sigma_{\theta,\,p_1^i}^2+\sigma_{\theta,\,p_2^i}^2}}\right)\right)}\xrightarrow{\theta} \max
    \end{split}
\end{equation}</div>
<p>There are different ways of finding the optimal (or sub-optimal) <span class="math">\(\theta\)</span> that maximizes <span class="caps">MLE</span> equation, 
but we’ll use a classic optimization approach widely used in machine learning — <em>gradient descent</em> (or ascent, 
depending on whether you strive for minimizing or maximizing a certain function). Its idea is quite simple:</p>
<ol>
<li>We start with a random (or arbitrary) parameter set <span class="math">\(\theta:=\theta_0\)</span>.</li>
<li>Then we calculate the gradient of the target function over the parameter set — <span class="math">\(\nabla \mathcal{L}\left(\,\theta \; ; \; \mathcal{D} \,\right)\)</span>; it is the “direction” and the rate of the fastest increase at the point <span class="math">\(\theta\)</span>. This direction is a vector, and has the same shape as <span class="math">\(\theta\)</span>.</li>
<li>Finally, we update the current point <span class="math">\(\theta\)</span> <em>slightly</em> moving it towards the gradient direction (or opposite, depending on whether we’re looking for maximal or minimal value of the target function), and repeat the process until a certain stop criterion is reached (e.g., number of iterations).</li>
</ol>
<p>Why do we do it <em>slightly</em>? Well, because we don’t want to overshoot the optima. A gradient in a certain point only 
gives us the direction and the rate of the fastest increase at <strong>that</strong> point. And gradients can change <em>quite a lot</em> 
as we move through the parameter space.
So we take a small step in the gradient direction (or the opposite one), and then recalculate the gradient at the new 
point, and so on.</p>
<p><img alt="Gradient Descent" class="image-process-article-image" src="https://vzhukov.dev/images/004/derivatives/article-image/gradient_descent.png"/></p>
<div class="caption">
<p>Gradient descent, as seen by ChatGPT</p>
</div>
<h3>Calculating Gradients</h3>
<p>Here will be a lot of math equations, so we’ll start with some notations and observations to make everything a bit 
easier to read. First, for standard normal distribution, we have:</p>
<div class="math">\begin{equation*}
    \begin{split}
        &amp; \phi(x) = \frac{1}{\sqrt{2\pi}}e^{-x^2/2} \; \text{ is a probability density function (PDF)} \\
        &amp; \Phi(x) = \int_{-\infty}^{x} \phi(t) dt  \; \text{ is a cumulative distribution function (CDF)} \\
        &amp; \frac{\partial}{\partial x}\Phi(x) = \phi(x) \; \text{ is a derivative of the CDF} \\
        &amp; \frac{\partial}{\partial x} \Phi\left(f(x)\right) = \phi\left(f(x)\right) \cdot \frac{\partial f(x)}{\partial x}  \; \text{ using a chain rule} \\
    \end{split}
\end{equation*}</div>
<p>Second, let’s recap that our target in <span class="caps">MLE</span> equation is to maximize the likelihood. But it’s the same as <strong>maximizing 
the logarithm of the likelihood</strong> (since logarithm is a monotonically increasing function), so we can take a 
logarithm of the whole equation and maximize it instead! This is a common practice in statistics, 
and it makes the math a bit easier.</p>
<div class="math">\begin{equation}
    \begin{split}
        &amp; \log{\mathcal{L}\left(\,\theta \; ; \; \mathcal{D} \,\right)} = \log{\prod_{i=1}^{m}{\left(1-\Phi\left(-\frac{\mu_{\theta,\, p_1^i}-\mu_{\theta,\,p_2^i}}{\sqrt{\sigma_{\theta,\,p_1^i}^2+\sigma_{\theta,\,p_2^i}^2}}\right)\right)}} = \\
        &amp; = \sum_{i=1}^{m}{\log{\left(1-\Phi\left(-\frac{\mu_{\theta,\, p_1^i}-\mu_{\theta,\,p_2^i}}{\sqrt{\sigma_{\theta,\,p_1^i}^2+\sigma_{\theta,\,p_2^i}^2}}\right)\right)}} \xrightarrow{\theta} \max
    \end{split}
\end{equation}</div>
<p>To find the gradient of the log likelihood function, we need to differentiate it with respect to the parameters 
<span class="math">\(\theta\)</span>. The gradient is a vector of partial derivatives of the log-likelihood function with respect to each parameter in <span class="math">\(\theta\)</span>. As we have <span class="math">\(2n\)</span> parameters, we will have <span class="math">\(2n\)</span> partial derivatives; <span class="math">\(n\)</span> partial derivatives with respect to 
<span class="math">\(\mu_i\)</span> and <span class="math">\(n\)</span> partial derivatives with respect to <span class="math">\(\sigma_i\)</span>:</p>
<div class="math">\begin{equation}
    \nabla \log {\mathcal{L}\left(\,\theta \; ; \; \mathcal{D} \,\right)} = \begin{pmatrix}
        \frac{\partial \log {\mathcal{L}\left(\,\theta \; ; \; \mathcal{D} \,\right)}}{\partial \mu_1} &amp; \frac{\partial \log {\mathcal{L}\left(\,\theta \; ; \; \mathcal{D} \,\right)}}{\partial \sigma_1}\\
        \\
        \frac{\partial \log {\mathcal{L}\left(\,\theta \; ; \; \mathcal{D} \,\right)}}{\partial \mu_2} &amp; \frac{\partial \log {\mathcal{L}\left(\,\theta \; ; \; \mathcal{D} \,\right)}}{\partial \sigma_2}\\
        \\
        \vdots &amp; \vdots\\
        \\
        \frac{\partial \log {\mathcal{L}\left(\,\theta \; ; \; \mathcal{D} \,\right)}}{\partial \mu_n} &amp; \frac{\partial \log {\mathcal{L}\left(\,\theta \; ; \; \mathcal{D} \,\right)}}{\partial \sigma_n}
    \end{pmatrix}
\end{equation}</div>
<p>Let’s calculate partial derivatives with respect to <span class="math">\(\mu_i\)</span> and <span class="math">\(\sigma_i\)</span>, and see what happens.</p>
<h4>Partial derivative with respect to means</h4>
<div class="math">\begin{equation}
    \begin{split}
        &amp; \frac{\partial}{\partial \mu_i} \log {\mathcal{L}\left(\,\theta \; ; \; \mathcal{D} \,\right)} = \frac{\partial}{\partial \mu_i} \sum_{k=1}^{m}{\log{\left(1-\Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)\right)}} = \\
        &amp; = \sum_{k=1}^{m}{\frac{\partial}{\partial \mu_i} \log{\left(1-\Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)\right)}} = \\
        &amp; = \sum_{k=1}^{m}{\frac{1}{1-\Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)} \cdot \frac{\partial}{\partial \mu_i} {\left(1-\Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)\right)}} = \\
        &amp; = \sum_{k=1}^{m}{\frac{-1}{1-\Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)} \cdot \frac{\partial}{\partial \mu_i} \Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)} \\
    \end{split} 
\end{equation}</div>
<p>Here are a couple of things worth noting. First, if player <span class="math">\(p_i\)</span> does not participate in a game <span class="math">\(G^j\)</span>, then the 
gradient of this game’s likelihood with respect to <span class="math">\(\mu_i\)</span> will be <span class="math">\(0\)</span>. In other words, for a player <span class="math">\(p_i\)</span> we should 
consider only games that involved this player. Second, as you can see in the second multiplier above, the difference 
between taking gradients with respect to <span class="math">\(\mu_{p^k_1}\)</span> and <span class="math">\(\mu_{p^k_2}\)</span> is only in the sign of the whole expression.</p>
<p>That means we can split equation into two parts - games where player <span class="math">\(p_i\)</span> won (we’ll denote them as <span class="math">\(G^i_{won}\)</span>), and games where they lost (<span class="math">\(G^i_{lost}\)</span>):</p>
<div class="math">\begin{equation}\label{eq:gradmu:calculated}
    \begin{split}
        &amp; \sum_{k=1}^{m}{\frac{-1}{1-\Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)} \cdot \frac{\partial}{\partial \mu_i} \Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)} = \\ 
        &amp; = \sum_{k \in G^i_{won}}{\frac{-1}{1-\Phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)} \cdot \frac{\partial}{\partial \mu_i} \Phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)} + \\
        &amp; + \sum_{k \in G^i_{lost}}{\frac{-1}{1-\Phi\left(-\frac{\mu_{p^k_1}-\mu_i}{\sqrt {\sigma_{p^k_1}^2+\sigma_i^2}}\right)} \cdot \frac{\partial}{\partial \mu_i} \Phi\left(-\frac{\mu_{p^k_1}-\mu_i}{\sqrt {\sigma_{p^k_1}^2+\sigma_i^2}}\right)} = \\
        &amp; = \sum_{k \in G^i_{won}}{\frac{-1}{1-\Phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)} \cdot \phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right) \cdot \frac{-1}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}} + \\
        &amp; + \sum_{k \in G^i_{lost}}{\frac{-1}{1-\Phi\left(-\frac{\mu_{p^k_1}-\mu_i}{\sqrt {\sigma_{p^k_1}^2+\sigma_i^2}}\right)} \cdot \phi\left(-\frac{\mu_{p^k_1}-\mu_i}{\sqrt {\sigma_{p^k_1}^2+\sigma_i^2}}\right) \cdot \frac{1}{\sqrt {\sigma_{p^k_1}^2+\sigma_i^2}}} = \\
        &amp; = \sum_{k \in G^i_{won}}{\frac{\phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)}{1-\Phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)} \cdot \frac{1}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}} - \sum_{k \in G^i_{lost}}{\frac{\phi\left(-\frac{\mu_{p^k_1}-\mu_i}{\sqrt {\sigma_{p^k_1}^2+\sigma_i^2}}\right)}{1-\Phi\left(-\frac{\mu_{p^k_1}-\mu_i}{\sqrt {\sigma_{p^k_1}^2+\sigma_i^2}}\right)} \cdot \frac{1}{\sqrt {\sigma_{p^k_1}^2+\sigma_i^2}}}
    \end{split}
\end{equation}</div>
<p>Looks terrifying, but it’ll be much easier to read in the code (a lot of that is already implemented in the <code>numpy</code> library).</p>
<h4>Partial derivative with respect to scales</h4>
<div class="math">\begin{equation}\label{eq:gradsig:all}
    \begin{split}
        &amp; \frac{\partial}{\partial \sigma_i} \log {\mathcal{L}\left(\,\theta \; | \; \mathcal{D} \,\right)} = \frac{\partial}{\partial \sigma_i} \sum_{k=1}^{m}{\log{\left(1-\Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)\right)}} = \\
        &amp; = \sum_{k=1}^{m}{\frac{\partial}{\partial \sigma_i} \log{\left(1-\Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)\right)}} = \\
        &amp; = \sum_{k=1}^{m}{\frac{1}{1-\Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)} \cdot \frac{\partial}{\partial \sigma_i} {\left(1-\Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)\right)}} = \\
        &amp; = \sum_{k=1}^{m}{\frac{-1}{1-\Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)} \cdot \frac{\partial}{\partial \sigma_i} \Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)} \\
    \end{split}
\end{equation}</div>
<p>It’s pretty much the same as for <span class="math">\(\mu_i\)</span>. And the observations are even more favorable here:</p>
<ul>
<li>Just as before, if player <span class="math">\(p_i\)</span> doesn’t participate in game <span class="math">\(G^j\)</span>, then the gradient of this game’s likelihood with respect to <span class="math">\(\sigma_i\)</span> will also be <span class="math">\(0\)</span>.</li>
<li>Moreover, <em>there is no difference</em> (in equation terms) between taking gradients with respect to <span class="math">\(\sigma_{p^k_1}\)</span> and <span class="math">\(\sigma_{p^k_2}\)</span>, since they both appear only in the denominator <span class="math">\(\sqrt{\sigma_{p^k_1}^2 + \sigma_{p^k_2}^2}\)</span>, and with the same sign and power.</li>
</ul>
<p>So we can consider only games where player <span class="math">\(p_i\)</span> participated, regardless of whether they won or lost:</p>
<div class="math">\begin{equation}\label{eq:gradsigma:calculated}
    \begin{split}
        &amp; \sum_{k=1}^{m}{\frac{-1}{1-\Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)} \cdot \frac{\partial}{\partial \sigma_i} \Phi\left(-\frac{\mu_{\theta,\, p_1^k}-\mu_{\theta,\,p_2^k}}{\sqrt{\sigma_{\theta,\,p_1^k}^2+\sigma_{\theta,\,p_2^k}^2}}\right)} = \\
        &amp; = \sum_{k \in G^i}{\frac{-1}{1-\Phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)} \cdot \frac{\partial}{\partial \sigma_i} \Phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)} = \\
        &amp; = \sum_{k \in G^i}{
        \frac{\mu_i-\mu_{p^k_2}}{1-\Phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)}
        \cdot
        \phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)
        \cdot
        \frac{\partial}{\partial \sigma_i}
        \left(\frac{1}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)
        } = \\
        &amp; = \sum_{k \in G^i}{
        \frac{\mu_i-\mu_{p^k_2}}{1-\Phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)}
        \cdot
        \phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)
        \cdot
        \left(-\frac{1}{2}\right)
        \cdot
        \frac{1}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}
        \cdot
        \frac{1}{\sigma_i^2+\sigma_{p^k_2}^2}
        \cdot
        2\sigma_i
        } = \\
        &amp; = \sum_{k \in G^i}{
        \frac{\phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)}{1-\Phi\left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)}
        \cdot
        \left(-\frac{\mu_i-\mu_{p^k_2}}{\sqrt {\sigma_i^2+\sigma_{p^k_2}^2}}\right)
        \cdot
        \frac{\sigma_i}{\sigma_i^2+\sigma_{p^k_2}^2}
        }
    \end{split}
\end{equation}</div>
<p>Interesting observation here: If player <span class="math">\(p_i\)</span> won against <span class="math">\(p_j\)</span>, and <span class="math">\(\mu_i &gt; \mu_j\)</span>, then their gradient with respect to <span class="math">\(\sigma_i\)</span> will be <em>negative</em> – meaning that optimally we’re reinforcing this difference; if it’s the other way 
around (e.g., <span class="math">\(p_i\)</span> lost to <span class="math">\(p_j\)</span>, but <span class="math">\(\mu_i &gt; \mu_j\)</span>), then the gradient with respect to <span class="math">\(\sigma_i\)</span> will be positive, 
meaning that “we found an unexpected case, and want to account for it by broadening the possible skills of player <span class="math">\(p_i\)</span>”.</p>
<p>Another interesting thing is that there are many multipliers appearing repeatedly in these equations, and it makes the 
whole implementation much easier from a programming perspective.</p>
<h3>Optimization algorithm</h3>
<p>It was already described above, but let’s recap it in a more formal manner:</p>
<ol>
<li>Given a set of players <span class="math">\(\mathcal{P}\)</span>, games dataset <span class="math">\(\mathcal{D}\)</span>, <a href="https://en.wikipedia.org/wiki/Learning_rate">learning rate</a> <span class="math">\(\alpha\)</span>, maximum number of optimization steps <span class="math">\(\text{max_iterations}\)</span>:</li>
<li><span class="math">\(\theta := \theta_0\)</span> (arbitrary, or random)</li>
<li><span class="math">\(\text{iteration} = 1\)</span></li>
<li>while <span class="math">\(\text{iteration} \leq \text{max_iterations}\)</span> (or while the magnitude of change is greater than a certain threshold):<ul>
<li>Calculate gradients, <span class="math">\(\nabla \log \mathcal{L}\left(\,\theta \; | \; \mathcal{D} \,\right)\)</span></li>
<li>Log gradient magnitude, <span class="math">\(||\nabla \log \mathcal{L}\left(\,\theta \; | \; \mathcal{D} \,\right)||\)</span> at the current step</li>
<li>Update parameters <span class="math">\(\theta = \theta\;+\;\alpha\,\cdot\,\nabla \log \mathcal{L}\left(\,\theta \; | \; \mathcal{D} \,\right)\)</span> (we’re using “gradient ascent” since we’re interested in maximizing the likelihood)</li>
<li>Log log-likelihood, <span class="math">\(\log\mathcal{L}\left(\,\theta \; | \; \mathcal{D} \,\right)\)</span></li>
<li><span class="math">\(\text{iteration} \;\;+=\;\;1\)</span></li>
</ul>
</li>
</ol>
<p><span class="dquo">“</span>Logging” steps are a good way to see what’s happening in our training process – we’ll plot charts showing how 
likelihood and gradient magnitude evolve over iterations.</p>
<p>In the <a href="https://vzhukov.dev/posts/2025/fitting-the-player-ranking-model-a-maximum-likelihood-approach">next post</a>, we will implement this 
algorithm in Python, and see how it works in practice.</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>
</section>
<footer class="post-info">
    Written by Viacheslav Zhukov in <a href="https://vzhukov.dev/posts/category/math">Math</a> on
    <time class="published" datetime="2025-06-12T14:00:00+02:00">        12th
June 2025
    </time>
</footer>
<section class="tags">
<a href="https://vzhukov.dev/posts/tag/gradient-descent">Gradient Descent</a>
<a href="https://vzhukov.dev/posts/tag/mathematical-modeling">Mathematical Modeling</a>
<a href="https://vzhukov.dev/posts/tag/maximum-likelihood-estimation">Maximum Likelihood Estimation</a>
<a href="https://vzhukov.dev/posts/tag/normal-distribution">Normal Distribution</a>
<a href="https://vzhukov.dev/posts/tag/player-ranking">Player Ranking</a>
<a href="https://vzhukov.dev/posts/tag/table-tennis">Table Tennis</a>
</section>
<section class="related-posts">
<h3>More like this</h3>
<ul id="related-posts">
<li><a href="https://vzhukov.dev/posts/2025/fitting-the-player-ranking-model-a-maximum-likelihood-approach">Fitting the Player Ranking Model: A Maximum Likelihood Approach</a></li>
<li><a href="https://vzhukov.dev/posts/2025/approximating-skills-of-table-tennis-players-using-normal-distribution-introduction">Approximating Skills of Table Tennis Players Using Normal Distribution. Introduction</a></li>
<li><a href="https://vzhukov.dev/posts/2025/data-grounded-country-comparison-the-service-idea">Data-Grounded Country Comparison: The Service Idea</a></li>
</ul>
</section>
</main>
<aside id="sidebar">
<div class="sidebar-block">
<img alt="Photo of Viacheslav Zhukov" class="profile-photo" src="https://vzhukov.dev/profile_photo.png"/>
<p>Doing AI &amp; ML engineering @ <a href="https://www.linkedin.com/company/toloka/" rel="noopener noreferrer" target="_blank" title="Toloka">Toloka</a></p>
<p> Occasional blogger, researcher, and math lover.</p>
<div class="social-icons">
<a href="https://www.linkedin.com/in/zhukpm/" rel="noopener noreferrer" target="_blank" title="LinkedIn profile of Viacheslav Zhukov">
<img alt="LinkedIn icon" class="social-icon" src="https://www.linkedin.com/favicon.ico"/>
</a>
<a href="https://github.com/zhukpm" rel="noopener noreferrer" target="_blank" title="GitHub profile of Viacheslav Zhukov">
<img alt="GitHub icon" class="social-icon" src="https://github.com/favicon.ico"/>
</a>
<a href="https://stackoverflow.com/users/6372685/viacheslav-zhukov" rel="noopener noreferrer" target="_blank" title="StackOverflow profile of Viacheslav Zhukov">
<img alt="StackOverflow icon" class="social-icon" src="https://stackoverflow.com/favicon.ico"/>
</a>
<a href="https://leetcode.com/perrymason/" rel="noopener noreferrer" target="_blank" title="LeetCode profile of Viacheslav Zhukov">
<img alt="LeetCode icon" class="social-icon" src="https://leetcode.com/favicon.ico"/>
</a>
</div>
</div>
<div class="sidebar-block">
<h2>Privacy</h2>
<p>
                        This site uses Google Analytics to understand visitor traffic and improve content.
                        It collects anonymous data like country, language, and pages visited - no personal
                        information. By continuing to browse, you agree to this minimal tracking.
                        <a href="https://www.google.com/search?q=is+google+analytics+tracking+safe" rel="noopener noreferrer" target="_blank">Read more</a>.</p>
</div>
<div class="sidebar-block">
<h2>Categories</h2>
<ul>
<li>
<a href="https://vzhukov.dev/posts/category/math">Math (2)</a>
</li>
<li>
<a href="https://vzhukov.dev/posts/category/natural-language-processing">Natural Language Processing (2)</a>
</li>
<li>
<a href="https://vzhukov.dev/posts/category/software-engineering">Software Engineering (2)</a>
</li>
<li>
<a href="https://vzhukov.dev/posts/category/ai-ml-based-applications">AI &amp; ML Based Applications (1)</a>
</li>
<li>
<a href="https://vzhukov.dev/posts/category/machine-learning">Machine Learning (1)</a>
</li>
</ul>
</div>
<div class="sidebar-block">
<h2>Recent posts</h2>
<ul>
<li>
<a href="https://vzhukov.dev/posts/2025/data-grounded-country-comparison-the-service-idea">Data-Grounded Country Comparison: The Service Idea</a>
</li>
<li>
<a href="https://vzhukov.dev/posts/2025/essential-tools-for-machine-learning-engineers-my-real-world-stack">Essential Tools for Machine Learning Engineers: My Real-World Stack</a>
</li>
<li>
<a href="https://vzhukov.dev/posts/2025/fitting-the-player-ranking-model-a-maximum-likelihood-approach">Fitting the Player Ranking Model: A Maximum Likelihood Approach</a>
</li>
<li>
<a href="https://vzhukov.dev/posts/2025/what-i-have-learned-during-my-1275-day-streak-on-leetcode">What I have learned during my 1275+ day streak on LeetCode</a>
</li>
<li>
<a href="https://vzhukov.dev/posts/2025/mathematical-model-for-player-ranking">Mathematical Model for Player Ranking</a>
</li>
<li>
<a href="https://vzhukov.dev/posts/2025/approximating-skills-of-table-tennis-players-using-normal-distribution-introduction">Approximating Skills of Table Tennis Players Using Normal Distribution. Introduction</a>
</li>
<li>
<a href="https://vzhukov.dev/posts/2023/text-classification-challenge-with-extra-small-datasets-fine-tuning-versus-chatgpt">Text classification challenge with extra-small datasets: Fine-tuning versus ChatGPT</a>
</li>
<li>
<a href="https://vzhukov.dev/posts/2023/choosing-the-best-architecture-for-your-text-classification-task">Choosing the best architecture for your text classification task</a>
</li>
</ul>
</div>
</aside>
<footer id="site-footer">
<p>Built with <a href="https://getpelican.com/" rel="noopener noreferrer" target="_blank">Pelican</a> using <a href="https://python.org/" rel="noopener noreferrer" target="_blank">Python</a> and <a href="https://github.com/hrw/pelican-haerwu-theme/" rel="noopener noreferrer" target="_blank">Haerwu theme</a>.</p>
<p>Copyright by Viacheslav Zhukov. Licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener noreferrer" target="_blank">CC BY-NC-SA 4.0</a>.</p>
</footer>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.30.0/prism.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.30.0/plugins/autoloader/prism-autoloader.min.js"></script>
<script src="https://cdn.jsdelivr.net/combine/npm/prismjs@1.30.0/plugins/toolbar/prism-toolbar.min.js,npm/prismjs@1.30.0/plugins/copy-to-clipboard/prism-copy-to-clipboard.min.js"></script>
</body>
</html>